{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import,  division , print_function\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[2.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.matmul(x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello,[[4.]]\n"
     ]
    }
   ],
   "source": [
    "print('hello,{}'.format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1,2],[3,4]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#broadcasting \n",
    "b = tf.add(a,1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2  6]\n",
      " [12 20]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  6]\n",
      " [12 20]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "c = np.multiply(a,b)\n",
    "print(c)\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizzbuz(max_num):\n",
    "    counter = tf.constant(0)\n",
    "    for num in range(max_num):\n",
    "        num = tf.constant(num)\n",
    "        if int(num%3)==0 and int(num % 5)==0:\n",
    "            print('FizzBuzz')\n",
    "        elif int(num % 3 ) == 0:\n",
    "            print('Fizz')\n",
    "        else:\n",
    "            print(num)\n",
    "        counter  += 1\n",
    "    return counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySimpleLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_units):\n",
    "        self.output_units = output_units\n",
    "\n",
    "    def build(self, input):\n",
    "        \n",
    "    # The build method gets called the first time your layer is used.\n",
    "    # Creating variables on build() allows you to make their shape depend\n",
    "    # on the input shape and hence remove the need for the user to specify\n",
    "    # full shapes. It is possible to create variables during __init__() if\n",
    "    # you already know their full shapes.\n",
    "        self.kernel = self.add_variable(          \n",
    "            \"kernel\", [input.shape[-1], self.output_units])\n",
    "\n",
    "    def call(self, input):\n",
    "    # Override call() instead of __call__ so we can perform some bookkeeping.\n",
    "        return tf.matmul(input, self.kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, input_shape=(784,)),  # must declare input shape\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=10)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, input):\n",
    "        \"\"\"Run the model.\"\"\"\n",
    "        result = self.dense1(input)\n",
    "        result = self.dense2(result)\n",
    "        result = self.dense2(result)  # reuse variables from dense2 layer\n",
    "        return result\n",
    "\n",
    "model = MNISTModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=152482, shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "w  = tfe.Variable([[1.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = w * w\n",
    "    \n",
    "grad = tape.gradient(loss,[w])\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss 69.559\n",
      "Loss at step 000 : 68.683\n",
      "Loss at step 020 : 55.583\n",
      "Loss at step 040 : 48.066\n",
      "Loss at step 060 : 43.599\n",
      "Loss at step 080 : 40.867\n",
      "Loss at step 100 : 39.160\n",
      "Loss at step 120 : 38.077\n",
      "Loss at step 140 : 37.381\n",
      "Loss at step 160 : 36.932\n",
      "Loss at step 180 : 36.639\n",
      "Final loss: 36.456\n",
      "W =2.953416347503662, B= 7.9534196853637695\n"
     ]
    }
   ],
   "source": [
    "# a toy dataset of points around 3 * x + 2 \n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise \n",
    "\n",
    "def prediction(input, weight, bias):\n",
    "    return input * weight + bias \n",
    "\n",
    "# A loss function using mean squared error\n",
    "def loss(weights,biases):\n",
    "    error = prediction(training_inputs,weights,biases) - training_outputs\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# Return the derivative of loss with respect to weight and bias\n",
    "def grad(weights, biases):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value  = loss(weights, biases)\n",
    "    return tape.gradient(loss_value,[weights,biases])\n",
    "\n",
    "train_steps  =  200\n",
    "learning_rate = 0.01\n",
    "# start with arbitary values for W and B on the same batch of data\n",
    "W = tfe.Variable(5.)\n",
    "B = tfe.Variable(10.)\n",
    "\n",
    "\n",
    "print(\"Initial loss {:.3f}\".format(loss(W,B)))\n",
    "\n",
    "for i in range(train_steps):\n",
    "    dW, dB = grad(W,B)\n",
    "    W.assign_sub(dW * learning_rate)\n",
    "    B.assign_sub(dW * learning_rate)\n",
    "    if i % 20 == 0 : \n",
    "        print(\"Loss at step {:03d} : {:.3f}\".format(i,loss(W,B)))\n",
    "        \n",
    "print(\"Final loss: {:.3f}\".format(loss(W,B)))\n",
    "print(\"W ={}, B= {}\".format(W.numpy(),B.numpy()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss : 70.271\n",
      "Loss at step 000: 67.490\n",
      "Loss at step 020: 30.318\n",
      "Loss at step 040: 13.952\n",
      "Loss at step 060: 6.741\n",
      "Loss at step 080: 3.561\n",
      "Loss at step 100: 2.158\n",
      "Loss at step 120: 1.539\n",
      "Loss at step 140: 1.265\n",
      "Loss at step 160: 1.144\n",
      "Loss at step 180: 1.091\n",
      "Loss at step 200: 1.067\n",
      "Loss at step 220: 1.057\n",
      "Loss at step 240: 1.052\n",
      "Loss at step 260: 1.050\n",
      "Loss at step 280: 1.049\n",
      "Final loss: 1.049\n",
      "W =2.9589593410491943, B=2.011157989501953\n"
     ]
    }
   ],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.W = tfe.Variable(5., name='weight')\n",
    "        self.B = tfe.Variable(10., name='bias')\n",
    "    def predict(self, inputs):\n",
    "        return inputs * self.W + self.B\n",
    "\n",
    "# a toy dataset of points around 3 * x + 2 \n",
    "NUM_EXAMPLES = 2000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise \n",
    "\n",
    "# The loss function to be optimized \n",
    "def loss(model, inputs, targets):\n",
    "    error = model.predict(inputs) - targets\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model,inputs,targets)\n",
    "    return tape.gradient(loss_value, [model.W, model.B])\n",
    "\n",
    "#Define : \n",
    "# 1.  A model\n",
    "# 2. Derivatives of a loss function with respect to model parameters \n",
    "# 3. A strategy for updating the variables based on  the derivatives \n",
    "writer = tf.contrib.summary.create_file_writer('logdir/')\n",
    "global_step=tf.train.get_or_create_global_step() \n",
    "\n",
    "\n",
    "\n",
    "model = Model() \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "print(\"Initial loss : {:.3f}\".format(loss(model,training_inputs, training_outputs)))\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for i in range(300):\n",
    "    grads = grad(model, training_inputs, training_outputs)\n",
    "    optimizer.apply_gradients(zip(grads, [model.W, model.B]),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        print (\"Loss at step {:03d}: {:.3f}\".format(i,loss(model, training_inputs,training_outputs)))\n",
    "        \n",
    "        \n",
    "print(\"Final loss: {:.3f}\".format(loss(model,training_inputs,training_outputs)))\n",
    "print(\"W ={}, B={}\".format(model.W.numpy(), model.B.numpy()))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>\n"
     ]
    }
   ],
   "source": [
    "# tfe.Checkpoint can save and restore tfe.Variables to and from checkpoints:\n",
    "\n",
    "x = tfe.Variable(10.)\n",
    "checkpoint = tfe.Checkpoint(x=x) # save a 'x'\n",
    "\n",
    "x.assign(2.) # assign a new value to the variables  and save.\n",
    "save_path = checkpoint.save('ckpt/')\n",
    "\n",
    "x.assign(11.) # change the variable after saving\n",
    "\n",
    "# restore values from the checkpoint\n",
    "checkpoint.restore(save_path)\n",
    "\n",
    "print(x) # --> 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.eager.python.checkpointable_utils.CheckpointLoadStatus at 0x7f9d60ab3978>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "model = Model()\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "checkpoint_dir = \"ckpt/\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "root = tfe.Checkpoint(optimizer=optimizer,\n",
    "                      model=model,\n",
    "                      optimizer_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "root.save(file_prefix=checkpoint_prefix)\n",
    "# or\n",
    "root.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=151581, shape=(), dtype=float64, numpy=2.5>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfe.metrics are stored as objects. \n",
    "#Update a metric by passing the new data to the callable,\n",
    "#and retrieve the result using the tfe.metrics.result method\n",
    "\n",
    "m = tfe.metrics.Mean('loss')\n",
    "m(0)\n",
    "m(5)\n",
    "m.result() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=151589, shape=(), dtype=int32, numpy=17>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m([8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=151611, shape=(), dtype=float32, numpy=1.0>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def log1pexp(x):\n",
    "  e = tf.exp(x)\n",
    "  def grad(dy):\n",
    "    return dy * (1 - 1 / (1 + e))\n",
    "  return tf.log(1 + e), grad\n",
    "\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# As before, the gradient computation works fine at x = 0.\n",
    "grad_log1pexp(0.)  # => [0.5]\n",
    "\n",
    "# And the gradient computation also works at x = 100.\n",
    "grad_log1pexp(100.)  # => [1.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to multiply a (1000, 1000) matrix by itself 200 times:\n",
      "CPU: 6.226000547409058 secs\n",
      "GPU: not found\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def measure(x, steps):\n",
    "  # TensorFlow initializes a GPU the first time it's used, exclude from timing.\n",
    "  tf.matmul(x, x)\n",
    "  start = time.time()\n",
    "  for i in range(steps):\n",
    "    x = tf.matmul(x, x)\n",
    "    _ = x.numpy()  # Make sure to execute op and not just enqueue it\n",
    "  end = time.time()\n",
    "  return end - start\n",
    "\n",
    "shape = (1000, 1000)\n",
    "steps = 200\n",
    "print(\"Time to multiply a {} matrix by itself {} times:\".format(shape, steps))\n",
    "\n",
    "# Run on CPU:\n",
    "with tf.device(\"/cpu:0\"):\n",
    "  print(\"CPU: {} secs\".format(measure(tf.random_normal(shape), steps)))\n",
    "\n",
    "# Run on GPU, if available:\n",
    "if tfe.num_gpus() > 0:\n",
    "  with tf.device(\"/gpu:0\"):\n",
    "    print(\"GPU: {} secs\".format(measure(tf.random_normal(shape), steps)))\n",
    "else:\n",
    "  print(\"GPU: not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tf.enable_eager_execution must be called at program startup.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-d64f35fb3d98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdivision\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/keras_tf_p3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution\u001b[0;34m(config, device_policy, execution_mode)\u001b[0m\n\u001b[1;32m   5466\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5467\u001b[0m     raise ValueError(\n\u001b[0;32m-> 5468\u001b[0;31m         \"tf.enable_eager_execution must be called at program startup.\")\n\u001b[0m\u001b[1;32m   5469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5470\u001b[0m   \u001b[0;31m# Monkey patch to get rid of an unnecessary conditional since the context is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
     ]
    }
   ],
   "source": [
    "\n",
    "def my_py_func(x):\n",
    "    x = tf.matmul(x, x)  # You can use tf ops\n",
    "    print(x)  # but it's eager!\n",
    "    return x\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    x = tf.placeholder(dtype=tf.float32)\n",
    "    # Call eager function in graph!\n",
    "    pf = tfe.py_func(my_py_func, [x], tf.float32)\n",
    "    sess.run(pf, feed_dict={x: [[2.0]]})  # [[4.0]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf_p3",
   "language": "python",
   "name": "keras_tf_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
