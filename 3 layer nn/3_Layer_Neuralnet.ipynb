{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source : https://medium.freecodecamp.org/building-a-3-layer-neural-network-from-scratch-99239c4af5d3\n",
    "\n",
    "# https://www.kaggle.com/daphnecor/week-1-3-layer-nn/code?scriptVersionId=2495447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "8bec90e6-23f2-4aae-8b4a-e9ef999391b7",
    "_uuid": "ae07d14dcb75d8188c49d57cdc54fc36f3397d02"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Cultivar 1</th>\n",
       "      <th>Cultivar 2</th>\n",
       "      <th>Cultivar 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.518613</td>\n",
       "      <td>-0.562250</td>\n",
       "      <td>0.232053</td>\n",
       "      <td>-1.169593</td>\n",
       "      <td>1.913905</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.034819</td>\n",
       "      <td>-0.659563</td>\n",
       "      <td>1.224884</td>\n",
       "      <td>0.251717</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>1.847920</td>\n",
       "      <td>1.013009</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.246290</td>\n",
       "      <td>-0.499413</td>\n",
       "      <td>-0.827996</td>\n",
       "      <td>-2.490847</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.568648</td>\n",
       "      <td>0.733629</td>\n",
       "      <td>-0.820719</td>\n",
       "      <td>-0.544721</td>\n",
       "      <td>-0.293321</td>\n",
       "      <td>0.406051</td>\n",
       "      <td>1.113449</td>\n",
       "      <td>0.965242</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196879</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>1.109334</td>\n",
       "      <td>-0.268738</td>\n",
       "      <td>0.088358</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.215533</td>\n",
       "      <td>-0.498407</td>\n",
       "      <td>2.135968</td>\n",
       "      <td>0.269020</td>\n",
       "      <td>0.318304</td>\n",
       "      <td>0.788587</td>\n",
       "      <td>1.395148</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.691550</td>\n",
       "      <td>-0.346811</td>\n",
       "      <td>0.487926</td>\n",
       "      <td>-0.809251</td>\n",
       "      <td>0.930918</td>\n",
       "      <td>2.491446</td>\n",
       "      <td>1.466525</td>\n",
       "      <td>-0.981875</td>\n",
       "      <td>1.032155</td>\n",
       "      <td>1.186068</td>\n",
       "      <td>-0.427544</td>\n",
       "      <td>1.184071</td>\n",
       "      <td>2.334574</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>1.840403</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>1.281985</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.663351</td>\n",
       "      <td>0.226796</td>\n",
       "      <td>0.401404</td>\n",
       "      <td>-0.319276</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.449601</td>\n",
       "      <td>-0.037874</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Alcohol  Malic acid       Ash  Alcalinity of ash  Magnesium  \\\n",
       "0  1.518613   -0.562250  0.232053          -1.169593   1.913905   \n",
       "1  0.246290   -0.499413 -0.827996          -2.490847   0.018145   \n",
       "2  0.196879    0.021231  1.109334          -0.268738   0.088358   \n",
       "3  1.691550   -0.346811  0.487926          -0.809251   0.930918   \n",
       "4  0.295700    0.227694  1.840403           0.451946   1.281985   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0       0.808997    1.034819             -0.659563         1.224884   \n",
       "1       0.568648    0.733629             -0.820719        -0.544721   \n",
       "2       0.808997    1.215533             -0.498407         2.135968   \n",
       "3       2.491446    1.466525             -0.981875         1.032155   \n",
       "4       0.808997    0.663351              0.226796         0.401404   \n",
       "\n",
       "   Color intensity       Hue  OD280/OD315 of diluted wines   Proline  \\\n",
       "0         0.251717  0.362177                      1.847920  1.013009   \n",
       "1        -0.293321  0.406051                      1.113449  0.965242   \n",
       "2         0.269020  0.318304                      0.788587  1.395148   \n",
       "3         1.186068 -0.427544                      1.184071  2.334574   \n",
       "4        -0.319276  0.362177                      0.449601 -0.037874   \n",
       "\n",
       "   Cultivar 1  Cultivar 2  Cultivar 3  \n",
       "0           1           0           0  \n",
       "1           1           0           0  \n",
       "2           1           0           0  \n",
       "3           1           0           0  \n",
       "4           1           0           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('W1data.csv')\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "5212493d-23b7-40d4-b1d6-6ce70a3631ca",
    "_uuid": "17597c0a6ba90e709149929aeca1999e2e74df32"
   },
   "outputs": [],
   "source": [
    "# Get the wine labels\n",
    "y = df[['Cultivar 1', 'Cultivar 2', 'Cultivar 3']].values\n",
    "\n",
    "# Get inputs; we define our x and y here.\n",
    "X = df.drop(['Cultivar 1', 'Cultivar 2', 'Cultivar 3'], axis = 1)\n",
    "X.shape, y.shape # Print shapes just to check\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "6b48153f-dd53-4bff-b04f-e6dfb036a59b",
    "_uuid": "6a2e1f0befbdb14f9085f58718ab64d9887e08a2"
   },
   "outputs": [],
   "source": [
    "#First we are importing all the libraries\n",
    "\n",
    "# Package imports\n",
    "# Matplotlib is a matlab like plotting library\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# SciKitLearn is a useful machine learning utilities library\n",
    "import sklearn\n",
    "# The sklearn dataset module helps generating datasets\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "90023c7a-8059-4117-98b3-82ceb3e5877a",
    "_uuid": "de660f4b64992b03558fc14ee176479b4b7afc29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 1.4504813381979211\n",
      "Accuracy after iteration 0 : 25.280898876404496 %\n",
      "Loss after iteration 100 : 0.5255210272876467\n",
      "Accuracy after iteration 100 : 80.89887640449437 %\n",
      "Loss after iteration 200 : 0.3964222056044325\n",
      "Accuracy after iteration 200 : 86.51685393258427 %\n",
      "Loss after iteration 300 : 0.3469405969185755\n",
      "Accuracy after iteration 300 : 87.64044943820225 %\n",
      "Loss after iteration 400 : 0.32062505328047924\n",
      "Accuracy after iteration 400 : 89.32584269662921 %\n",
      "Loss after iteration 500 : 0.30057660926569424\n",
      "Accuracy after iteration 500 : 89.32584269662921 %\n",
      "Loss after iteration 600 : 0.27757542390200524\n",
      "Accuracy after iteration 600 : 89.8876404494382 %\n",
      "Loss after iteration 700 : 0.25803463679942457\n",
      "Accuracy after iteration 700 : 90.4494382022472 %\n",
      "Loss after iteration 800 : 0.2451896091466233\n",
      "Accuracy after iteration 800 : 90.4494382022472 %\n",
      "Loss after iteration 900 : 0.23453472996993888\n",
      "Accuracy after iteration 900 : 90.4494382022472 %\n",
      "Loss after iteration 1000 : 0.224901853756763\n",
      "Accuracy after iteration 1000 : 90.4494382022472 %\n",
      "Loss after iteration 1100 : 0.21649115993849932\n",
      "Accuracy after iteration 1100 : 92.13483146067416 %\n",
      "Loss after iteration 1200 : 0.20897333560828457\n",
      "Accuracy after iteration 1200 : 93.25842696629213 %\n",
      "Loss after iteration 1300 : 0.20186399701140648\n",
      "Accuracy after iteration 1300 : 93.25842696629213 %\n",
      "Loss after iteration 1400 : 0.18022550179248037\n",
      "Accuracy after iteration 1400 : 93.25842696629213 %\n",
      "Loss after iteration 1500 : 0.1669018228661396\n",
      "Accuracy after iteration 1500 : 93.82022471910112 %\n",
      "Loss after iteration 1600 : 0.15681341901851129\n",
      "Accuracy after iteration 1600 : 94.3820224719101 %\n",
      "Loss after iteration 1700 : 0.14891431157061535\n",
      "Accuracy after iteration 1700 : 94.9438202247191 %\n",
      "Loss after iteration 1800 : 0.14240653665707972\n",
      "Accuracy after iteration 1800 : 94.9438202247191 %\n",
      "Loss after iteration 1900 : 0.1357845617374409\n",
      "Accuracy after iteration 1900 : 95.50561797752809 %\n",
      "Loss after iteration 2000 : 0.1319146672331526\n",
      "Accuracy after iteration 2000 : 95.50561797752809 %\n",
      "Loss after iteration 2100 : 0.1290070757552617\n",
      "Accuracy after iteration 2100 : 95.50561797752809 %\n",
      "Loss after iteration 2200 : 0.1266137159634668\n",
      "Accuracy after iteration 2200 : 95.50561797752809 %\n",
      "Loss after iteration 2300 : 0.12454049447039091\n",
      "Accuracy after iteration 2300 : 95.50561797752809 %\n",
      "Loss after iteration 2400 : 0.12268163837704688\n",
      "Accuracy after iteration 2400 : 95.50561797752809 %\n",
      "Loss after iteration 2500 : 0.12096337880182337\n",
      "Accuracy after iteration 2500 : 95.50561797752809 %\n",
      "Loss after iteration 2600 : 0.11931607604440037\n",
      "Accuracy after iteration 2600 : 95.50561797752809 %\n",
      "Loss after iteration 2700 : 0.11764906434462091\n",
      "Accuracy after iteration 2700 : 96.06741573033707 %\n",
      "Loss after iteration 2800 : 0.11578680890335502\n",
      "Accuracy after iteration 2800 : 96.06741573033707 %\n",
      "Loss after iteration 2900 : 0.11312910792526903\n",
      "Accuracy after iteration 2900 : 96.06741573033707 %\n",
      "Loss after iteration 3000 : 0.1051221602698985\n",
      "Accuracy after iteration 3000 : 96.06741573033707 %\n",
      "Loss after iteration 3100 : 0.08528325019384515\n",
      "Accuracy after iteration 3100 : 97.19101123595506 %\n",
      "Loss after iteration 3200 : 0.07310937002597405\n",
      "Accuracy after iteration 3200 : 98.31460674157303 %\n",
      "Loss after iteration 3300 : 0.07094849409554872\n",
      "Accuracy after iteration 3300 : 98.31460674157303 %\n",
      "Loss after iteration 3400 : 0.06647041651883503\n",
      "Accuracy after iteration 3400 : 98.31460674157303 %\n",
      "Loss after iteration 3500 : 0.06349694100556133\n",
      "Accuracy after iteration 3500 : 98.31460674157303 %\n",
      "Loss after iteration 3600 : 0.06209091813348392\n",
      "Accuracy after iteration 3600 : 98.87640449438202 %\n",
      "Loss after iteration 3700 : 0.06098723303288365\n",
      "Accuracy after iteration 3700 : 98.87640449438202 %\n",
      "Loss after iteration 3800 : 0.05989614213136659\n",
      "Accuracy after iteration 3800 : 98.87640449438202 %\n",
      "Loss after iteration 3900 : 0.05868449560544275\n",
      "Accuracy after iteration 3900 : 98.87640449438202 %\n",
      "Loss after iteration 4000 : 0.05727076686257899\n",
      "Accuracy after iteration 4000 : 98.87640449438202 %\n",
      "Loss after iteration 4100 : 0.0554444294154591\n",
      "Accuracy after iteration 4100 : 98.87640449438202 %\n",
      "Loss after iteration 4200 : 0.05188248794580268\n",
      "Accuracy after iteration 4200 : 98.87640449438202 %\n",
      "Loss after iteration 4300 : 0.0437258167110182\n",
      "Accuracy after iteration 4300 : 99.43820224719101 %\n",
      "Loss after iteration 4400 : 0.0385472561531449\n",
      "Accuracy after iteration 4400 : 99.43820224719101 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc83d078cc0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc83d01d780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we define all our functions\n",
    "\n",
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_loss(y,y_hat):\n",
    "    # Clipping value\n",
    "    minval = 0.000000000001\n",
    "    # Number of samples\n",
    "    m = y.shape[0]\n",
    "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss\n",
    "\n",
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n",
    "\n",
    "#FORWARD PROPAGATION\n",
    "# This is the forward propagation function\n",
    "def forward_prop(model,a0):\n",
    "\n",
    "    \n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
    "    \n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + our bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    #Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return cache\n",
    "\n",
    "# This is the backward propagation function\n",
    "def backward_prop(model,cache,y):\n",
    "\n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
    "\n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n",
    "\n",
    "#TRAINING PHASE\n",
    "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
    "    # First layer weights\n",
    "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
    "    \n",
    "    # First layer bias\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    \n",
    "    # Second layer weights\n",
    "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
    "    \n",
    "    # Second layer bias\n",
    "    b2 = np.zeros((1, nn_hdim))\n",
    "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
    "    b3 = np.zeros((1,nn_output_dim))\n",
    "    \n",
    "    \n",
    "    # Package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
    "    return model\n",
    "\n",
    "def update_parameters(model,grads,learning_rate):\n",
    "    # Load parameters\n",
    "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    \n",
    "    # Store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "    return model\n",
    "\n",
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = np.argmax(c['a3'], axis=1)\n",
    "    return y_hat\n",
    "\n",
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "losses = []\n",
    "\n",
    "def train(model,X_,y_,learning_rate, epochs=20000, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        # Pring loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            a3 = cache['a3']\n",
    "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
    "            y_hat = predict(model,X_)\n",
    "            y_true = y_.argmax(axis=1)\n",
    "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
    "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
    "    return model\n",
    "np.random.seed(0)\n",
    "# This is what we return at the end\n",
    "model = initialize_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\n",
    "model = train(model,X,y,learning_rate=0.07,epochs=4500,print_loss=True)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cf85a92b-2bc6-4b57-8cd6-53b523add48a",
    "_uuid": "6aca14a72b394b8bc0d83fae7134fd4f8bcbfce2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf_p3",
   "language": "python",
   "name": "keras_tf_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
