{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8165226178731431\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "\n",
    "#This program is just a rough reimplementation of logistic regression in Python. It's not particularly\n",
    "#optimized in any way but it does give a sense of backpropagation, computing the loss function, and\n",
    "#updating the weights. The derivatives are taken numerically, instead of analytically. Numeric\n",
    "#derivatives are a bit slower and less intuitive, but do still work in this case.\n",
    "\n",
    "Xtrain = [5,6,10,7,4]\n",
    "Ytrain = [1,1,0,0,1] #Stores the training labels\n",
    "\n",
    "#Hyperparameters \n",
    "numTrainingExamples = 5\n",
    "learningRate = .1\n",
    "numEpochs =1000\n",
    "\n",
    "#In this case, our hypothesis is in the form of a model representing logistic\n",
    "#regression\n",
    "def hypothesis(t0,t1,x):\n",
    "\treturn (1/(1+np.exp(-(t0 + t1*x))))\n",
    "\n",
    "#Our loss function is different depending on whether the y value is 0 or 1\n",
    "def costFunction(t0, t1):\n",
    "\tloss = 0\n",
    "\tfor i, j in zip(Xtrain,Ytrain):\n",
    "\t\ttemp = (-j*math.log(hypothesis(t0,t1,i))) - (1-j)*math.log(1 - hypothesis(t0,t1,i))\n",
    "\t\tloss += temp\n",
    "\treturn loss/numTrainingExamples\n",
    "\n",
    "#Weight updates are done by taking the derivative of the loss function \n",
    "#with respect to each of the theta values. \n",
    "def weightUpdate(withRespectTo, t0, t1):\n",
    "\tif (withRespectTo == \"theta0\"):\n",
    "\t\tt0 = t0 - learningRate*(derivative(withRespectTo, t0, t1))\n",
    "\t\treturn t0\n",
    "\telse: #has to be wrt to theta1\n",
    "\t\tt1 = t1 - learningRate*(derivative(withRespectTo, t0, t1))\n",
    "\t\treturn t1\n",
    "\t\n",
    "#Evaluating a numerical deerivative\n",
    "def derivative(withRespectTo, t0, t1):\n",
    "\th = 1./1000.\n",
    "\tif (withRespectTo == \"theta0\"):\n",
    "\t\trise = costFunction(t0 + h, t1) - costFunction(t0,t1)\n",
    "\telse: #has to be wrt to theta1\n",
    "\t\trise = costFunction(t0 , t1 + h) - costFunction(t0,t1)\n",
    "\trun = h\n",
    "\tslope = rise/run\n",
    "\treturn slope\n",
    "\n",
    "#Random initialization of the theta values\n",
    "theta0 = np.random.uniform(0,1)\n",
    "theta1 = np.random.uniform(0,1)\n",
    "#Test value\n",
    "Xtest = 5\n",
    "for i in range(0,numEpochs):\n",
    "\ttheta0 = weightUpdate(\"theta0\", theta0, theta1)\n",
    "\ttheta1 = weightUpdate(\"theta1\", theta0, theta1)\n",
    "print (hypothesis(theta0,theta1,Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf",
   "language": "python",
   "name": "keras_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
